{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune on multiple problem\n"
      ],
      "metadata": {
        "id": "eYIo_9nnL234"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertTokenizerFast,\n",
        "    BertForSequenceClassification,\n",
        "    BertForTokenClassification,\n",
        "    BertForQuestionAnswering,\n",
        "    get_linear_schedule_with_warmup ###### Gradually warms up then decays learning rate for stable BERT training.\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.optim import AdamW #Adam Optimizer with Weight Decay"
      ],
      "metadata": {
        "id": "J3zw1qZzL5mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "_4qUzl8DL9pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "bgBq8VRYL_cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT Text Classifier\n",
        "class BERTTextClassifier:\n",
        "    \"\"\"BERT for Text Classification (Sentiment, Spam etc.)\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='bert-base-uncased', num_classes=2, max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.num_classes = num_classes\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\n",
        "            model_name, num_labels=num_classes\n",
        "        )\n",
        "        self.model.to(device)\n",
        "\n",
        "    def load_imdb_data(self, sample_size=5000):\n",
        "\n",
        "        \"\"\"Load IMDb movie reviews dataset\"\"\"\n",
        "\n",
        "        print(\"Loading IMDb dataset...\")\n",
        "\n",
        "        dataset = load_dataset(\"imdb\")\n",
        "\n",
        "        # Sample data for faster training\n",
        "        train_indices = np.random.choice(len(dataset['train']),\n",
        "                                       min(sample_size, len(dataset['train'])),\n",
        "                                       replace=False)\n",
        "\n",
        "        test_indices = np.random.choice(len(dataset['test']),\n",
        "                                      min(sample_size//4, len(dataset['test'])),\n",
        "                                      replace=False)\n",
        "\n",
        "        # Convert numpy.int64 → int for indexing\n",
        "        train_texts = [dataset['train'][int(i)]['text'] for i in train_indices]\n",
        "        train_labels = [dataset['train'][int(i)]['label'] for i in train_indices]\n",
        "\n",
        "        test_texts = [dataset['test'][int(i)]['text'] for i in test_indices]\n",
        "        test_labels = [dataset['test'][int(i)]['label'] for i in test_indices]\n",
        "\n",
        "        print(f\"Train samples: {len(train_texts)}\")\n",
        "        print(f\"Test samples: {len(test_texts)}\")\n",
        "\n",
        "        return train_texts, train_labels, test_texts, test_labels\n",
        "\n",
        "    def train(self, train_texts, train_labels, epochs=1, batch_size=8, learning_rate=2e-5):\n",
        "\n",
        "        \"\"\"Train the text classifier\"\"\"\n",
        "\n",
        "        train_dataset = TextClassificationDataset(\n",
        "            train_texts, train_labels, self.tokenizer, self.max_length\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "\n",
        "        total_steps = len(train_loader) * epochs\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "\n",
        "            for batch in progress_bar:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                loss = outputs.loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                scheduler.step()\n",
        "\n",
        "                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "            print(f'Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "    def evaluate(self, test_texts, test_labels, batch_size=8):\n",
        "\n",
        "        \"\"\"Evaluate the text classifier\"\"\"\n",
        "\n",
        "        test_dataset = TextClassificationDataset(\n",
        "            test_texts, test_labels, self.tokenizer, self.max_length\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        true_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc='Evaluating'):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "                predictions.extend(preds)\n",
        "                true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "        report = classification_report(true_labels, predictions,\n",
        "                                     target_names=['Negative', 'Positive'])\n",
        "\n",
        "        return accuracy, f1, report\n",
        "\n",
        "    def predict(self, texts):\n",
        "\n",
        "        \"\"\"Predict sentiment for new texts\"\"\"\n",
        "        predictions = []\n",
        "        probabilities = []\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        for text in texts:\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            input_ids = encoding['input_ids'].to(device)\n",
        "            attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits\n",
        "                probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "                pred = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "                predictions.append(pred)\n",
        "                probabilities.append(probs)\n",
        "\n",
        "        return predictions, probabilities"
      ],
      "metadata": {
        "id": "ar2GuMQqMZvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NERDataset Class Details\n",
        "\n",
        "- Tokens: [\"John\", \"lives\", \"in\", \"London\"]\n",
        "\n",
        "- Labels: [1, 0, 0, 2]\n",
        "\n",
        "    - 1 = B-PER (John is a person)\n",
        "\n",
        "    - 0 = O (Outside entity)\n",
        "\n",
        "    - 2 = B-LOC (London is a location)\n",
        "\n",
        "\n",
        "**Step 1: Tokenizer output**\n",
        "\n",
        "- Original words: John | lives | in | London\n",
        "\n",
        "- BERT tokens: [CLS], John, lives, in, Lon, ##don, [SEP], [PAD]...\n",
        "\n",
        "- Word IDs: None, 0, 1, 2, 3, 3, None, None...\n",
        "\n",
        "**Step 2: Label Alignment**\n",
        "\n",
        "- Input IDs: [101, 1001, 2002, 1999, 3001, 3010, 102, 0, 0, 0]\n",
        "\n",
        "- Word IDs: [None, 0, 1, 2, 3, 3, None, None, None, None]\n",
        "\n",
        "- Aligned Labels:[-100, 1, 0, 0, 2, -100, -100, -100, -100, -100]\n",
        "\n",
        "**Step 3: Final output that goes to model**\n",
        "\n",
        "- { 'input_ids': tensor([...]), # token IDs\n",
        "\n",
        "- 'attention_mask': tensor([1,1,1,...]), # 1 for real tokens, 0 for pads\n",
        "\n",
        "- 'labels': tensor([-100,1,0,0,2,-100,-100,...]) # aligned labels }"
      ],
      "metadata": {
        "id": "Ea-Le7YpOqXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## NERDatatset Class\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    \"\"\"Dataset for Named Entity Recognition\"\"\"\n",
        "\n",
        "    def __init__(self, tokens_list, labels_list, tokenizer, max_length=512):\n",
        "        self.tokens_list = tokens_list\n",
        "        self.labels_list = labels_list\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.tokens_list[idx]\n",
        "        labels = self.labels_list[idx]\n",
        "\n",
        "        # Tokenize with word-level alignment\n",
        "        encoding = self.tokenizer(\n",
        "            tokens,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            is_split_into_words=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Get word alignment for batch index 0\n",
        "        word_ids = encoding.word_ids(batch_index=0)\n",
        "        aligned_labels = []\n",
        "        previous_word_idx = None\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                aligned_labels.append(-100)  # Ignore special tokens\n",
        "            elif word_idx != previous_word_idx:\n",
        "                aligned_labels.append(labels[word_idx] if word_idx < len(labels) else 0)\n",
        "            else:\n",
        "                aligned_labels.append(-100)  # Ignore subword tokens\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        # Ensure aligned_labels length matches max_length\n",
        "        if len(aligned_labels) < self.max_length:\n",
        "            aligned_labels += [-100] * (self.max_length - len(aligned_labels))\n",
        "        elif len(aligned_labels) > self.max_length:\n",
        "            aligned_labels = aligned_labels[:self.max_length]\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),  # Shape: (max_length,)\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),  # Shape: (max_length,)\n",
        "            'labels': torch.tensor(aligned_labels, dtype=torch.long)  # Shape: (max_length,)\n",
        "        }"
      ],
      "metadata": {
        "id": "i_Jvsi6tM-At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERTNERClassifer NER(Name Entity Recognition) Details\n",
        "\n",
        "| **Label**  | **Full Form**          | **Meaning (Hindi + English)**                                          | **Example**                                                       |\n",
        "| ---------- | ---------------------- | ---------------------------------------------------------------------- | ----------------------------------------------------------------- |\n",
        "| **O**      | Outside                | Means no Entity (normal word)                                      | \"works\", \"at\"                                                     |\n",
        "| **B-PER**  | Begin - Person         | Person entity first name (starting word of someone's name)                         | \"John\" → `B-PER`                                                  |\n",
        "| **I-PER**  | Inside - Person        | Continuation word Person entity (second name of a person)                    | \"Mary Jane\" → `Mary = B-PER`, `Jane = I-PER`                      |\n",
        "| **B-ORG**  | Begin - Organization   | Organization/company first word                                     | \"Google\" → `B-ORG`                                                |\n",
        "| **I-ORG**  | Inside - Organization  | Organization's remaining name after first word                                   | \"New York Times\" → `New = B-ORG`, `York = I-ORG`, `Times = I-ORG` |\n",
        "| **B-LOC**  | Begin - Location       | Location/place first word                                           | \"London\" → `B-LOC`                                                |\n",
        "| **I-LOC**  | Inside - Location      | Location's rest of the words                                        | \"New York\" → `New = B-LOC`, `York = I-LOC`                        |\n",
        "| **B-MISC** | Begin - Miscellaneous  | Miscellaneous entity's first word (event, product, nationality, etc.) | \"Indian\" (nationality) → `B-MISC`                                 |\n",
        "| **I-MISC** | Inside - Miscellaneous | Miscellaneous entity second/rest of the word (if is multi-word)              | \"South Korean\" → `South = B-MISC`, `Korean = I-MISC`              |\n",
        "\n",
        "\n",
        "- B → Begin (entity starting word)\n",
        "\n",
        "- I → Inside (entity continuation words)\n",
        "\n",
        "- O → Outside (no entity, normal word)\n",
        "\n",
        "WikiAnn (Wikipedia + Annotation) dataset\n",
        "\n",
        "**Complete Flow:**\n",
        "\n",
        "- Load the WikiAnn English Dataset from the Hugging Face.\n",
        "- Randomly select samples : training (1000) and test (250).\n",
        "- Separate out the Tokens and its NER tags in the separate lists.\n",
        "- Return the data so that it can be used in the training pipeline"
      ],
      "metadata": {
        "id": "wl3sxFPZPtxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTNERClassifier:\n",
        "    \"\"\"BERT for Named Entity Recognition\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='bert-base-uncased', num_labels=9, max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.num_labels = num_labels\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "        self.model = BertForTokenClassification.from_pretrained(\n",
        "            model_name, num_labels=num_labels\n",
        "        )\n",
        "        self.model.to(device)\n",
        "\n",
        "        # Label mapping\n",
        "        self.labels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG',\n",
        "                       'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "\n",
        "    def load_wikiann_data(self, sample_size=1000):\n",
        "        \"\"\"Load WikiAnn (Wikipedia + Annotation) dataset\"\"\"\n",
        "        print(\"Loading wikiann NER dataset...\")\n",
        "        dataset = load_dataset(\"wikiann\", \"en\")\n",
        "\n",
        "        train_indices = np.random.choice(len(dataset['train']),\n",
        "                                       min(sample_size, len(dataset['train'])),\n",
        "                                       replace=False)\n",
        "\n",
        "        test_indices = np.random.choice(len(dataset['test']),\n",
        "                                      min(sample_size//4, len(dataset['test'])),\n",
        "                                      replace=False)\n",
        "\n",
        "        # Convert numpy.int64 to int\n",
        "        train_tokens = [dataset['train'][int(i)]['tokens'] for i in train_indices]\n",
        "        train_labels = [dataset['train'][int(i)]['ner_tags'] for i in train_indices]\n",
        "\n",
        "        test_tokens = [dataset['test'][int(i)]['tokens'] for i in test_indices]\n",
        "        test_labels = [dataset['test'][int(i)]['ner_tags'] for i in test_indices]\n",
        "\n",
        "        print(f\"Train samples: {len(train_tokens)}\")\n",
        "        print(f\"Test samples: {len(test_tokens)}\")\n",
        "\n",
        "        return train_tokens, train_labels, test_tokens, test_labels\n",
        "\n",
        "    def train(self, train_tokens, train_labels, epochs=1, batch_size=8, learning_rate=2e-5):\n",
        "        \"\"\"Train the NER model\"\"\"\n",
        "        train_dataset = NERDataset(\n",
        "            train_tokens, train_labels, self.tokenizer, self.max_length\n",
        "        )\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "        total_steps = len(train_loader) * epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "\n",
        "            for batch in progress_bar:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                loss = outputs.loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "            print(f'Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "    def evaluate(self, test_tokens, test_labels, batch_size=8):\n",
        "        \"\"\"Evaluate the NER model\"\"\"\n",
        "        test_dataset = NERDataset(\n",
        "            test_tokens, test_labels, self.tokenizer, self.max_length\n",
        "        )\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        self.model.eval()\n",
        "        predictions = []\n",
        "        true_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc='Evaluating'):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits\n",
        "                preds = torch.argmax(logits, dim=2).cpu().numpy()\n",
        "                labels = labels.cpu().numpy()\n",
        "\n",
        "                # Collect valid predictions (ignore -100)\n",
        "                for i in range(preds.shape[0]):\n",
        "                    for j in range(preds.shape[1]):\n",
        "                        if labels[i][j] != -100:\n",
        "                            predictions.append(preds[i][j])\n",
        "                            true_labels.append(labels[i][j])\n",
        "\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "\n",
        "        return accuracy, f1\n",
        "\n",
        "    def predict(self, tokens_list):\n",
        "        \"\"\"Predict NER tags for new tokens\"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        self.model.eval()\n",
        "        for tokens in tokens_list:\n",
        "            encoding = self.tokenizer(\n",
        "                tokens,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt',\n",
        "                is_split_into_words=True\n",
        "            )\n",
        "\n",
        "            input_ids = encoding['input_ids'].to(device)\n",
        "            attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits\n",
        "                preds = torch.argmax(logits, dim=2).cpu().numpy()[0]\n",
        "\n",
        "                # Get alignment for original words\n",
        "                word_ids = encoding.word_ids(batch_index=0)\n",
        "                token_predictions = []\n",
        "                previous_word_idx = None\n",
        "\n",
        "                for i, word_idx in enumerate(word_ids):\n",
        "                    if word_idx is not None and word_idx != previous_word_idx:\n",
        "                        if word_idx < len(tokens):\n",
        "                            token_predictions.append(self.labels[preds[i]])\n",
        "                    previous_word_idx = word_idx\n",
        "\n",
        "                predictions.append(token_predictions)\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "Yp_pqQsoNU3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QADataset(Dataset):\n",
        "    \"\"\"Dataset for Question Answering (SQuAD-style)\"\"\"\n",
        "\n",
        "    def __init__(self, questions, contexts, answers, tokenizer, max_length=512):\n",
        "        self.questions = questions\n",
        "        self.contexts = contexts\n",
        "        self.answers = answers\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        context = self.contexts[idx]\n",
        "        answer = self.answers[idx]  # dict: {'text': [...], 'answer_start': [...]}\n",
        "\n",
        "        # Encode inputs with offsets to locate answer\n",
        "        encoding = self.tokenizer(\n",
        "            question,\n",
        "            context,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_offsets_mapping=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        offset_mapping = encoding.pop(\"offset_mapping\")[0]  # (max_length, 2)\n",
        "        start_positions = torch.tensor(0, dtype=torch.long)\n",
        "        end_positions = torch.tensor(0, dtype=torch.long)\n",
        "\n",
        "        if answer and 'answer_start' in answer and answer['answer_start']:\n",
        "            answer_start = answer['answer_start'][0]\n",
        "            answer_text = answer['text'][0]\n",
        "            answer_end = answer_start + len(answer_text)\n",
        "\n",
        "            # Find token start/end matching answer char positions\n",
        "            for idx, (start, end) in enumerate(offset_mapping):\n",
        "                if start <= answer_start < end:\n",
        "                    start_positions = torch.tensor(idx, dtype=torch.long)\n",
        "                if start < answer_end <= end:\n",
        "                    end_positions = torch.tensor(idx, dtype=torch.long)\n",
        "                    break\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),      # (max_length,)\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'start_positions': start_positions,\n",
        "            'end_positions': end_positions\n",
        "        }"
      ],
      "metadata": {
        "id": "dkRqkvF-RltN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTQuestionAnswering:\n",
        "    \"\"\"BERT for Question Answering (SQuAD)\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "        self.model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "        self.model.to(device)\n",
        "\n",
        "    def load_squad_data(self, sample_size=2000):\n",
        "        \"\"\"Load and sample SQuAD dataset\"\"\"\n",
        "        print(\"Loading SQuAD dataset...\")\n",
        "        dataset = load_dataset(\"squad\")\n",
        "\n",
        "        train_indices = np.random.choice(len(dataset['train']),\n",
        "                                         min(sample_size, len(dataset['train'])),\n",
        "                                         replace=False)\n",
        "        val_indices = np.random.choice(len(dataset['validation']),\n",
        "                                       min(sample_size//4, len(dataset['validation'])),\n",
        "                                       replace=False)\n",
        "\n",
        "        train_questions = [dataset['train'][int(i)]['question'] for i in train_indices]\n",
        "        train_contexts = [dataset['train'][int(i)]['context'] for i in train_indices]\n",
        "        train_answers = [dataset['train'][int(i)]['answers'] for i in train_indices]\n",
        "\n",
        "        val_questions = [dataset['validation'][int(i)]['question'] for i in val_indices]\n",
        "        val_contexts = [dataset['validation'][int(i)]['context'] for i in val_indices]\n",
        "        val_answers = [dataset['validation'][int(i)]['answers'] for i in val_indices]\n",
        "\n",
        "        print(f\"Train samples: {len(train_questions)}\")\n",
        "        print(f\"Validation samples: {len(val_questions)}\")\n",
        "\n",
        "        return (train_questions, train_contexts, train_answers,\n",
        "                val_questions, val_contexts, val_answers)\n",
        "\n",
        "    def train(self, questions, contexts, answers, epochs=1, batch_size=8, learning_rate=2e-5):\n",
        "        \"\"\"Train QA model using QADataset (offset mapping based)\"\"\"\n",
        "        train_dataset = QADataset(questions, contexts, answers, self.tokenizer, self.max_length)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "\n",
        "        total_steps = len(train_loader) * epochs\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "\n",
        "            for batch in progress_bar:\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                start_positions = batch['start_positions'].to(device)\n",
        "                end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    start_positions=start_positions,\n",
        "                    end_positions=end_positions\n",
        "                )\n",
        "\n",
        "                loss = outputs.loss\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                scheduler.step()\n",
        "\n",
        "                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "            print(f'Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "    def answer_question(self, question, context, max_answer_len=30):\n",
        "        \"\"\"Answer a single question given context\"\"\"\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            question,\n",
        "            context,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].to(device)\n",
        "\n",
        "        attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            start_logits = outputs.start_logits\n",
        "\n",
        "            end_logits = outputs.end_logits\n",
        "\n",
        "            start_idx = torch.argmax(start_logits, dim=1).item()\n",
        "\n",
        "            end_idx = torch.argmax(end_logits, dim=1).item()\n",
        "\n",
        "            # Ensure valid span\n",
        "            if end_idx < start_idx:\n",
        "                end_idx = start_idx\n",
        "\n",
        "            if (end_idx - start_idx) > max_answer_len:\n",
        "                end_idx = start_idx + max_answer_len\n",
        "\n",
        "            # Decode predicted tokens\n",
        "            answer_tokens = input_ids[0][start_idx:end_idx+1]\n",
        "\n",
        "            answer = self.tokenizer.decode(answer_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "            return answer"
      ],
      "metadata": {
        "id": "Cjx2fM5NRtZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_text_classification_demo():\n",
        "\n",
        "    \"\"\"Demo for text classification\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TEXT CLASSIFICATION (Sentiment Analysis) DEMO\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    classifier = BERTTextClassifier(num_classes=2)\n",
        "\n",
        "    # Load data\n",
        "    train_texts, train_labels, test_texts, test_labels = classifier.load_imdb_data(sample_size=1000)\n",
        "\n",
        "    # Show sample\n",
        "    print(f\"\\nSample Review: {train_texts[0][:200]}...\")\n",
        "    print(f\"Label: {'Positive' if train_labels[0] == 1 else 'Negative'}\")\n",
        "\n",
        "    # Train for 2 epochs (small for demo)\n",
        "    classifier.train(train_texts, train_labels, epochs=1, batch_size=8)\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy, f1, report = classifier.evaluate(test_texts, test_labels, batch_size=8)\n",
        "\n",
        "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Test custom examples\n",
        "    custom_reviews = [\n",
        "        \"This movie was fantastic! Amazing acting and great plot.\",\n",
        "        \"Boring and terrible. Waste of time.\",\n",
        "        \"Not bad, could be better though.\"\n",
        "    ]\n",
        "\n",
        "    predictions, probabilities = classifier.predict(custom_reviews)\n",
        "\n",
        "    print(f\"\\nCustom Predictions:\")\n",
        "\n",
        "    for text, pred, prob in zip(custom_reviews, predictions, probabilities):\n",
        "\n",
        "        sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
        "\n",
        "        confidence = prob[pred] * 100\n",
        "\n",
        "        print(f\"'{text[:50]}...' -> {sentiment} ({confidence:.1f}%)\")"
      ],
      "metadata": {
        "id": "uZoxYcQDRwUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ner_demo():\n",
        "\n",
        "    \"\"\"Demo for Named Entity Recognition\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"NAMED ENTITY RECOGNITION DEMO\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    ner_model = BERTNERClassifier(num_labels=9)\n",
        "\n",
        "    # Load small subset for demo (fast training)\n",
        "    train_tokens, train_labels, test_tokens, test_labels = ner_model.load_wikiann_data(sample_size=500)\n",
        "\n",
        "    # Show sample\n",
        "    print(f\"\\nSample tokens: {train_tokens[0][:10]}\")\n",
        "\n",
        "    label_names = [ner_model.labels[l] if l < len(ner_model.labels) else \"O\" for l in train_labels[0][:10]]\n",
        "\n",
        "    print(f\"Sample labels: {label_names}\")\n",
        "\n",
        "    # Train for 2 epochs\n",
        "    ner_model.train(train_tokens, train_labels, epochs=1, batch_size=8)\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy, f1 = ner_model.evaluate(test_tokens, test_labels, batch_size=8)\n",
        "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Custom examples\n",
        "    custom_sentences = [\n",
        "        [\"John\", \"Smith\", \"works\", \"at\", \"Google\", \"in\", \"California\"],\n",
        "        [\"Apple\", \"Inc.\", \"was\", \"founded\", \"by\", \"Steve\", \"Jobs\"]\n",
        "    ]\n",
        "\n",
        "    predictions = ner_model.predict(custom_sentences)\n",
        "    print(f\"\\nCustom Predictions:\")\n",
        "    for tokens, preds in zip(custom_sentences, predictions):\n",
        "        print(\"Tokens:\", tokens)\n",
        "        print(\"Labels:\", preds)\n",
        "        print()"
      ],
      "metadata": {
        "id": "G3EBxf0RRyhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_qa_demo():\n",
        "\n",
        "    \"\"\"Demo for Question Answering\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"QUESTION ANSWERING DEMO\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    qa_model = BERTQuestionAnswering()\n",
        "\n",
        "    # Load small subset (for speed)\n",
        "    (train_questions, train_contexts, train_answers,\n",
        "     val_questions, val_contexts, val_answers) = qa_model.load_squad_data(sample_size=500)\n",
        "\n",
        "    # Safe sample print\n",
        "    ans_text = train_answers[0]['text'][0] if train_answers[0]['text'] else 'No answer'\n",
        "\n",
        "    print(f\"\\nSample Question: {train_questions[0]}\")\n",
        "\n",
        "    print(f\"Sample Context: {train_contexts[0][:200]}...\")\n",
        "\n",
        "    print(f\"Sample Answer: {ans_text}\")\n",
        "\n",
        "    # Train model (2 epochs for demo)\n",
        "    qa_model.train(train_questions, train_contexts, train_answers, epochs=1, batch_size=4)\n",
        "\n",
        "    # Test on custom questions\n",
        "    print(f\"\\nCustom Q&A Examples:\")\n",
        "\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"question\": \"What is the capital of France?\",\n",
        "            \"context\": \"France is a country in Europe. Paris is the capital and largest city of France. The city is known for the Eiffel Tower and the Louvre Museum.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Who founded Apple?\",\n",
        "            \"context\": \"Apple Inc. is an American technology company. It was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976. The company is known for products like iPhone and Mac.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for case in test_cases:\n",
        "        answer = qa_model.answer_question(case[\"question\"], case[\"context\"], max_answer_len=30)\n",
        "        print(f\"Q: {case['question']}\")\n",
        "        print(f\"A: {answer}\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "PBqtMUR-R0dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BERT Multi-Task Demo\")\n",
        "print(\"Choose a task to run:\")\n",
        "print(\"1. Text Classification (Sentiment Analysis)\")\n",
        "print(\"2. Named Entity Recognition (NER)\")\n",
        "print(\"3. Question Answering\")\n",
        "print(\"4. Run All Tasks\")\n",
        "\n",
        "choice = input(\"\\nEnter your choice (1-4): \").strip()\n",
        "\n",
        "try:\n",
        "    if choice == \"1\":\n",
        "        run_text_classification_demo()\n",
        "    elif choice == \"2\":\n",
        "        run_ner_demo()\n",
        "    elif choice == \"3\":\n",
        "        run_qa_demo()\n",
        "    elif choice == \"4\":\n",
        "        run_text_classification_demo()\n",
        "        run_ner_demo()\n",
        "        run_qa_demo()\n",
        "    else:\n",
        "        print(\"Invalid choice! Please run again.\")\n",
        "except Exception as e:\n",
        "    print(\"\\n--- ERROR OCCURRED ---\")\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nMake sure you have:\")\n",
        "    print(\"1. Installed required packages:\")\n",
        "    print(\"   pip install torch transformers datasets scikit-learn tqdm numpy\")\n",
        "    print(\"2. Loaded all classes & dataset helpers (BERTTextClassifier, BERTNERClassifier, BERTQuestionAnswering, TextClassificationDataset, NERDataset, QADataset)\")\n",
        "    print(\"3. Using the fixed versions (with int casting, offset_mapping for QA, word_ids fix for NER)\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "mL6Kr8TBR2iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZSigZ_jR6HU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}