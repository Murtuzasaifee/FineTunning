{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MIiZ0G1Dug33"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. transforms.ToTensor()**\n",
        "\n",
        "MNIST images are originally in PIL Image format (28×28 grayscale).\n",
        "\n",
        "This converts them into a PyTorch Tensor, and also scales pixel values to the [0,1] range.\n",
        "\n",
        "Originally, pixel values are between 0–255.\n",
        "\n",
        "After ToTensor(), each pixel becomes pixel / 255.\n",
        "\n",
        "\n",
        "**2. transforms.Normalize((0.5,), (0.5,))**\n",
        "\n",
        "This further shifts and scales pixel values to the [-1, 1] range,\n",
        "\n",
        "Let say, x is the pixel value in [0,1] (after ToTensor()), then after applying the transforms.Nowmalize(), it will below:\n",
        "\n",
        "0 → -1\n",
        "\n",
        "0.5 → 0\n",
        "\n",
        "1 → +1\n",
        "\n",
        "We normalize like this because, Neural networks train better when inputs are centered around zero (mean ≈ 0).\n",
        "\n",
        "It helps speed up training and prevents issues like exploding/vanishing gradients."
      ],
      "metadata": {
        "id": "_PJpzGrjz1Q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Dataset (MNIST) -----\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ],
      "metadata": {
        "id": "tmsH-qrwyS0F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(test_data, batch_size=1000)"
      ],
      "metadata": {
        "id": "BOLyQpJXyXPL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Visualize the data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "fig, axes = plt.subplots(1, 8, figsize=(15, 2))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(images[i].squeeze(), cmap='gray')  # grayscale\n",
        "    ax.set_title(f\"Label: {labels[i].item()}\")\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "cf-0xvlPyifY",
        "outputId": "2e2c7a10-7eed-43fe-91c3-f9fe12379720"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x200 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Teacher Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ----- Teacher Model (2 hidden layers) -----\n",
        "class TeacherMLP(nn.Module):\n",
        "    def __init__(self, hidden1=512, hidden2=256):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden1, hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden2, 10)  # 10 output classes (digits)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "OezfblAAyndV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create instances\n",
        "teacher = TeacherMLP(hidden1=512, hidden2=256)\n",
        "\n",
        "print(teacher)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5X-devXyyaI",
        "outputId": "149afc92-f6b1-44db-e93d-c120fbfe67e3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TeacherMLP(\n",
            "  (net): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Train Teacher -----\n",
        "def train_teacher(model, loader, epochs=1, lr=1e-3):\n",
        "\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x, y in loader:\n",
        "            opt.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = loss_fn(out, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Teacher Epoch {ep+1}: Loss = {total_loss/len(loader):.4f}\")"
      ],
      "metadata": {
        "id": "nf99AGxXy3Jg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_teacher(teacher, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYdBl1oh8lq-",
        "outputId": "cf321f9d-0a84-44cd-d808-b77393cdb1f6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Epoch 1: Loss = 0.2997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## View updated weights\n",
        "print(teacher.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B7twt9E83uj",
        "outputId": "5e48635a-eac9-4500-b3bf-9d59d918d7c7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict({'net.1.weight': tensor([[ 0.0093, -0.0179,  0.0144,  ...,  0.0072,  0.0042,  0.0481],\n",
            "        [ 0.0237,  0.0079, -0.0020,  ..., -0.0216,  0.0173, -0.0258],\n",
            "        [ 0.0347,  0.0281, -0.0131,  ..., -0.0241, -0.0019,  0.0201],\n",
            "        ...,\n",
            "        [-0.0260, -0.0103,  0.0104,  ..., -0.0424,  0.0168,  0.0087],\n",
            "        [-0.0016, -0.0251,  0.0227,  ..., -0.0077,  0.0161, -0.0075],\n",
            "        [ 0.0104,  0.0029,  0.0318,  ...,  0.0024,  0.0120,  0.0012]]), 'net.1.bias': tensor([ 0.0056, -0.0409, -0.0156, -0.0043, -0.0194,  0.0025,  0.0088,  0.0285,\n",
            "        -0.0241, -0.0022, -0.0027, -0.0004,  0.0380, -0.0369,  0.0376,  0.0070,\n",
            "         0.0015, -0.0278,  0.0048,  0.0006, -0.0013, -0.0094,  0.0089, -0.0288,\n",
            "        -0.0040, -0.0160,  0.0278,  0.0068,  0.0055, -0.0263,  0.0297, -0.0427,\n",
            "         0.0005, -0.0254, -0.0286, -0.0069, -0.0326, -0.0284, -0.0357, -0.0006,\n",
            "         0.0150,  0.0089, -0.0281,  0.0230, -0.0370, -0.0138,  0.0024, -0.0385,\n",
            "        -0.0210,  0.0295,  0.0152, -0.0037, -0.0348,  0.0011,  0.0262,  0.0263,\n",
            "         0.0113,  0.0236,  0.0164, -0.0066,  0.0208,  0.0230, -0.0200, -0.0305,\n",
            "         0.0150, -0.0068,  0.0028, -0.0160,  0.0256,  0.0145, -0.0083, -0.0099,\n",
            "        -0.0280, -0.0179,  0.0065, -0.0257, -0.0468, -0.0366,  0.0291,  0.0286,\n",
            "         0.0013,  0.0235,  0.0016, -0.0381, -0.0313, -0.0100, -0.0262,  0.0202,\n",
            "         0.0180,  0.0130,  0.0199,  0.0234, -0.0131, -0.0283,  0.0068, -0.0367,\n",
            "         0.0282,  0.0249, -0.0270, -0.0185,  0.0262, -0.0132, -0.0086,  0.0106,\n",
            "         0.0210, -0.0419, -0.0310, -0.0132, -0.0274,  0.0306, -0.0237,  0.0148,\n",
            "         0.0156,  0.0094,  0.0220, -0.0173, -0.0340, -0.0105, -0.0244,  0.0051,\n",
            "         0.0025, -0.0212,  0.0235, -0.0326,  0.0346,  0.0289, -0.0178,  0.0241,\n",
            "         0.0064, -0.0263, -0.0124, -0.0090, -0.0165,  0.0016,  0.0216, -0.0005,\n",
            "         0.0037, -0.0003,  0.0383,  0.0305, -0.0340,  0.0320,  0.0117,  0.0195,\n",
            "        -0.0062, -0.0324, -0.0307,  0.0050, -0.0090,  0.0020,  0.0117,  0.0272,\n",
            "         0.0084,  0.0025,  0.0077,  0.0166, -0.0235,  0.0048, -0.0072, -0.0011,\n",
            "        -0.0066, -0.0446, -0.0214, -0.0139, -0.0036,  0.0145,  0.0119, -0.0292,\n",
            "         0.0213,  0.0039,  0.0086, -0.0244, -0.0152, -0.0025, -0.0192, -0.0146,\n",
            "         0.0036, -0.0103, -0.0272, -0.0162, -0.0368, -0.0204, -0.0236, -0.0181,\n",
            "        -0.0262, -0.0131,  0.0260, -0.0307, -0.0156, -0.0237,  0.0063,  0.0230,\n",
            "         0.0231, -0.0302,  0.0114,  0.0161,  0.0231, -0.0115, -0.0246, -0.0419,\n",
            "         0.0115, -0.0046, -0.0060, -0.0106,  0.0202,  0.0342,  0.0077,  0.0133,\n",
            "         0.0155, -0.0175, -0.0165, -0.0137, -0.0237,  0.0248, -0.0224, -0.0241,\n",
            "        -0.0129, -0.0095,  0.0101, -0.0375,  0.0047,  0.0246,  0.0194, -0.0227,\n",
            "        -0.0096, -0.0031,  0.0198,  0.0063,  0.0220,  0.0134,  0.0019,  0.0244,\n",
            "         0.0185,  0.0154,  0.0283, -0.0166, -0.0146, -0.0084,  0.0332, -0.0304,\n",
            "        -0.0275,  0.0029,  0.0166, -0.0033, -0.0053,  0.0282, -0.0466, -0.0108,\n",
            "         0.0019, -0.0018, -0.0031,  0.0173, -0.0389, -0.0225, -0.0010, -0.0140,\n",
            "        -0.0063,  0.0322, -0.0194, -0.0202,  0.0273, -0.0192, -0.0360, -0.0145,\n",
            "        -0.0175, -0.0206,  0.0295, -0.0125, -0.0222, -0.0013, -0.0377, -0.0314,\n",
            "        -0.0194, -0.0287,  0.0043,  0.0207, -0.0257, -0.0144, -0.0216,  0.0091,\n",
            "         0.0286, -0.0125,  0.0096, -0.0354, -0.0258,  0.0078,  0.0112,  0.0311,\n",
            "        -0.0369, -0.0133,  0.0178, -0.0179, -0.0046,  0.0217,  0.0072, -0.0020,\n",
            "        -0.0083,  0.0248,  0.0158, -0.0227,  0.0246,  0.0209, -0.0096,  0.0148,\n",
            "         0.0071,  0.0074,  0.0258,  0.0039,  0.0256, -0.0290, -0.0413,  0.0151,\n",
            "         0.0226,  0.0187, -0.0324, -0.0299, -0.0329,  0.0327,  0.0266,  0.0124,\n",
            "         0.0308,  0.0158,  0.0199,  0.0073,  0.0317,  0.0087,  0.0221,  0.0162,\n",
            "        -0.0022, -0.0127,  0.0126, -0.0357, -0.0219, -0.0295, -0.0013,  0.0126,\n",
            "        -0.0288, -0.0272,  0.0119,  0.0192, -0.0027, -0.0328, -0.0229, -0.0304,\n",
            "        -0.0244,  0.0108, -0.0154, -0.0081,  0.0155,  0.0168,  0.0243, -0.0294,\n",
            "         0.0195, -0.0230,  0.0262, -0.0357, -0.0230, -0.0013,  0.0027, -0.0274,\n",
            "        -0.0322, -0.0171, -0.0393,  0.0237, -0.0338,  0.0260,  0.0230, -0.0466,\n",
            "         0.0096, -0.0169, -0.0096,  0.0123,  0.0149, -0.0112, -0.0031, -0.0328,\n",
            "        -0.0030,  0.0337, -0.0004,  0.0251,  0.0127, -0.0002, -0.0183,  0.0342,\n",
            "        -0.0368, -0.0343,  0.0165, -0.0296,  0.0074,  0.0314, -0.0016, -0.0351,\n",
            "         0.0214, -0.0241, -0.0137, -0.0216, -0.0223,  0.0248, -0.0003,  0.0247,\n",
            "        -0.0084,  0.0085,  0.0036,  0.0255, -0.0337, -0.0394, -0.0079, -0.0362,\n",
            "         0.0007,  0.0187,  0.0024,  0.0238,  0.0141,  0.0265, -0.0219,  0.0197,\n",
            "        -0.0050, -0.0162,  0.0131,  0.0181, -0.0173,  0.0279, -0.0054, -0.0112,\n",
            "        -0.0251,  0.0211,  0.0038, -0.0311, -0.0360,  0.0056, -0.0404, -0.0174,\n",
            "        -0.0215, -0.0117, -0.0007, -0.0054,  0.0239,  0.0214,  0.0226, -0.0248,\n",
            "         0.0257,  0.0037,  0.0028, -0.0151, -0.0224, -0.0267, -0.0193, -0.0222,\n",
            "        -0.0012,  0.0170,  0.0151,  0.0351,  0.0118,  0.0251, -0.0315, -0.0062,\n",
            "         0.0186, -0.0359,  0.0127, -0.0199,  0.0304,  0.0131, -0.0356, -0.0336,\n",
            "        -0.0072, -0.0286, -0.0238, -0.0294,  0.0189, -0.0103,  0.0053, -0.0337,\n",
            "        -0.0384, -0.0151, -0.0320,  0.0091, -0.0384, -0.0436,  0.0176,  0.0154,\n",
            "         0.0153,  0.0127,  0.0106,  0.0155, -0.0249, -0.0168, -0.0094, -0.0141,\n",
            "         0.0094, -0.0219,  0.0210, -0.0082, -0.0265,  0.0038, -0.0331, -0.0173,\n",
            "         0.0237,  0.0268,  0.0247,  0.0271,  0.0357,  0.0233,  0.0097, -0.0068,\n",
            "        -0.0333,  0.0026,  0.0300, -0.0172, -0.0031,  0.0115,  0.0077, -0.0136]), 'net.3.weight': tensor([[-0.0997, -0.0417,  0.0058,  ...,  0.0146, -0.0103,  0.0125],\n",
            "        [ 0.0356,  0.0153, -0.0867,  ...,  0.0337,  0.0318,  0.0306],\n",
            "        [-0.0137, -0.0249, -0.0472,  ..., -0.0223, -0.0024,  0.0353],\n",
            "        ...,\n",
            "        [ 0.0479, -0.0376, -0.0245,  ..., -0.0348,  0.0184, -0.0248],\n",
            "        [-0.0231, -0.0223,  0.0013,  ...,  0.0309,  0.0386, -0.0026],\n",
            "        [-0.0407, -0.0485,  0.0536,  ...,  0.0142, -0.0165, -0.0311]]), 'net.3.bias': tensor([ 0.0451,  0.0239, -0.0415,  0.0042,  0.0359,  0.0130, -0.0422,  0.0053,\n",
            "         0.0425,  0.0168,  0.0084, -0.0178, -0.0466,  0.0472, -0.0105, -0.0198,\n",
            "         0.0258, -0.0011,  0.0327, -0.0206, -0.0355,  0.0253,  0.0144,  0.0087,\n",
            "        -0.0162,  0.0034, -0.0440,  0.0255,  0.0154, -0.0112, -0.0206,  0.0290,\n",
            "         0.0215,  0.0027, -0.0097,  0.0208, -0.0017, -0.0192,  0.0171, -0.0648,\n",
            "        -0.0190, -0.0295, -0.0137, -0.0576, -0.0205, -0.0368,  0.0024, -0.0079,\n",
            "         0.0414,  0.0391,  0.0049, -0.0172,  0.0371, -0.0131,  0.0568,  0.0096,\n",
            "         0.0265, -0.0254,  0.0467, -0.0265,  0.0155, -0.0743,  0.0336, -0.0374,\n",
            "         0.0447, -0.0036, -0.0082, -0.0496,  0.0155,  0.0402, -0.0348,  0.0064,\n",
            "        -0.0292,  0.0144,  0.0289,  0.0034, -0.0019, -0.0054, -0.0210,  0.0056,\n",
            "         0.0169, -0.0387, -0.0084, -0.0228,  0.0120,  0.0207, -0.0279, -0.0157,\n",
            "         0.0468, -0.0419, -0.0180, -0.0346,  0.0037, -0.0451,  0.0016,  0.0260,\n",
            "        -0.0197, -0.0093, -0.0210, -0.0400, -0.0490,  0.0324, -0.0260, -0.0181,\n",
            "        -0.0127,  0.0333,  0.0136,  0.0155,  0.0383, -0.0344,  0.0268,  0.0325,\n",
            "        -0.0151,  0.0268,  0.0337,  0.0084,  0.0488, -0.0308, -0.0031,  0.0009,\n",
            "        -0.0122, -0.0485,  0.0085, -0.0540, -0.0291,  0.0235,  0.0393,  0.0248,\n",
            "        -0.0223,  0.0371,  0.0190,  0.0133,  0.0505,  0.0188,  0.0419,  0.0395,\n",
            "        -0.0030,  0.0259, -0.0222,  0.0024,  0.0096, -0.0395,  0.0090,  0.0562,\n",
            "         0.0098, -0.0320, -0.0345, -0.0281,  0.0365,  0.0325,  0.0367,  0.0100,\n",
            "        -0.0552, -0.0396, -0.0390,  0.0389,  0.0174, -0.0120, -0.0243, -0.0114,\n",
            "         0.0010,  0.0011, -0.0142,  0.0003, -0.0029, -0.0206, -0.0098, -0.0412,\n",
            "        -0.0411, -0.0184,  0.0031,  0.0419,  0.0064, -0.0309, -0.0584,  0.0223,\n",
            "         0.0441,  0.0103,  0.0306,  0.0353, -0.0210,  0.0073,  0.0252,  0.0328,\n",
            "         0.0113,  0.0272, -0.0382, -0.0250, -0.0469, -0.0132, -0.0293, -0.0409,\n",
            "         0.0180,  0.0114,  0.0019, -0.0111, -0.0594, -0.0304,  0.0389, -0.0356,\n",
            "        -0.0165, -0.0643, -0.0072,  0.0100, -0.0372,  0.0277, -0.0250, -0.0128,\n",
            "         0.0501,  0.0328, -0.0112, -0.0089,  0.0242, -0.0174, -0.0516,  0.0582,\n",
            "         0.0052,  0.0117, -0.0319,  0.0346, -0.0317,  0.0214,  0.0309, -0.0040,\n",
            "        -0.0347, -0.0498,  0.0007,  0.0419,  0.0436,  0.0161,  0.0164,  0.0552,\n",
            "         0.0003, -0.0045,  0.0528,  0.0608, -0.0286, -0.0343,  0.0501, -0.0162,\n",
            "        -0.0031, -0.0362,  0.0084, -0.0330, -0.0344,  0.0017,  0.0114, -0.0030,\n",
            "        -0.0377, -0.0064, -0.0341,  0.0594, -0.0365,  0.0459, -0.0184,  0.0199]), 'net.5.weight': tensor([[-0.0115, -0.0815,  0.0590,  ..., -0.0146, -0.0304, -0.0094],\n",
            "        [-0.0969, -0.0051,  0.0431,  ..., -0.0408,  0.0321, -0.0638],\n",
            "        [-0.0316,  0.0421,  0.0100,  ...,  0.0223,  0.1085,  0.0290],\n",
            "        ...,\n",
            "        [ 0.0554, -0.0913, -0.0540,  ..., -0.0444,  0.0586,  0.0226],\n",
            "        [ 0.0543, -0.0386, -0.0215,  ..., -0.0155,  0.0647,  0.0072],\n",
            "        [-0.0321, -0.0698,  0.0537,  ..., -0.1232, -0.0502, -0.0606]]), 'net.5.bias': tensor([-0.0210, -0.0252,  0.0461,  0.0102, -0.0548,  0.0328,  0.0115, -0.0246,\n",
            "         0.0628,  0.0024])})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ----- Freeze Teacher -----\n",
        "# for param in teacher.parameters():\n",
        "#     param.requires_grad = False\n",
        "# teacher.eval()\n",
        "# print(\"Teacher model trained and frozen.\")"
      ],
      "metadata": {
        "id": "1HgAvQfP8_-5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Student Model (1 hidden layer) -----\n",
        "class StudentMLP(nn.Module):\n",
        "    def __init__(self, hidden=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 10)  # smaller model, only 1 hidden layer\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "uBZeoSqz9Iio"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student = StudentMLP(hidden=128)\n",
        "print(student)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw57VWja9Na3",
        "outputId": "d5f855b2-57f4-4fdb-8b16-4f5505b024b2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StudentMLP(\n",
            "  (net): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **There are two approaches:**\n",
        "\n",
        "1. PreTrain the student model with hard labels and then perform the distillation\n",
        "2. Do the knowledge distillation on student model (just the initialized model with some random weights) from the Teacher model soft labels"
      ],
      "metadata": {
        "id": "dsDwyoAE-JpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Approach 2\n",
        "\n",
        "# ----- Distillation Training -----\n",
        "temperature = 2.0\n",
        "alpha = 0.7\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "optimizer = optim.Adam(student.parameters(), lr=1e-3)\n",
        "\n",
        "def distill(student, teacher, loader, epochs=1):\n",
        "    for ep in range(epochs):\n",
        "\n",
        "        student.train()\n",
        "\n",
        "        total_loss = 0\n",
        "\n",
        "        for x, y in loader:\n",
        "            # Teacher outputs\n",
        "            with torch.no_grad():\n",
        "                t_logits = teacher(x)\n",
        "                t_probs = torch.softmax(t_logits / temperature, dim=1)\n",
        "\n",
        "            # Student outputs\n",
        "            s_logits = student(x)\n",
        "            s_log_probs = torch.log_softmax(s_logits / temperature, dim=1)\n",
        "\n",
        "            # Losses\n",
        "            loss_soft = kl_loss(s_log_probs, t_probs) * (temperature**2)\n",
        "            loss_hard = ce_loss(s_logits, y)\n",
        "            loss = alpha * loss_soft + (1 - alpha) * loss_hard\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Student Epoch {ep+1}: Loss = {total_loss/len(loader):.4f}\")"
      ],
      "metadata": {
        "id": "IeobFZpu9QzC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distill(student, teacher, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG5UDie4-sFl",
        "outputId": "8961e4c5-e96f-4787-d019-ec09f4f110ea"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Epoch 1: Loss = 0.5786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Evaluation Function -----\n",
        "def evaluate(model, loader, name=\"Model\"):\n",
        "    model.eval()\n",
        "\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            out = model(x)\n",
        "            preds = out.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    acc = correct / total * 100\n",
        "    print(f\"{name} Accuracy: {acc:.2f}%\")\n",
        "    return acc"
      ],
      "metadata": {
        "id": "abrMfzx6-4O0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate both Teacher and Student\n",
        "\n",
        "evaluate(teacher, test_loader, \"Teacher\")\n",
        "\n",
        "evaluate(student, test_loader, \"Student\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD6a7ZHI--Vz",
        "outputId": "f4a33ce9-854a-4e9f-b79f-497bbdc11ef2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Accuracy: 94.77%\n",
            "Student Accuracy: 92.96%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92.96"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Approach 1\n",
        "\n",
        "## Pretrain student on hard labels (optional warm-up)\n",
        "def pretrain_student(student, loader, epochs=1, lr=1e-3):\n",
        "    student.train()\n",
        "    opt = optim.Adam(student.parameters(), lr=lr)\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    for ep in range(epochs):\n",
        "        for x, y in loader:\n",
        "            opt.zero_grad()\n",
        "            out = student(x)\n",
        "            loss = ce_loss(out, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n"
      ],
      "metadata": {
        "id": "KykFQwCY_AS_"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrain_student(student, train_loader, epochs=1)"
      ],
      "metadata": {
        "id": "5DPPUKiFA8DO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Distillation Training -----\n",
        "temperature = 2.0\n",
        "alpha = 0.7\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "optimizer = optim.Adam(student.parameters(), lr=1e-3)\n",
        "\n",
        "def distill(student, teacher, loader, epochs=1):\n",
        "    for ep in range(epochs):\n",
        "\n",
        "        student.train()\n",
        "\n",
        "        total_loss = 0\n",
        "\n",
        "        for x, y in loader:\n",
        "            # Teacher outputs\n",
        "            with torch.no_grad():\n",
        "                t_logits = teacher(x)\n",
        "                t_probs = torch.softmax(t_logits / temperature, dim=1)\n",
        "\n",
        "            # Student outputs\n",
        "            s_logits = student(x)\n",
        "            s_log_probs = torch.log_softmax(s_logits / temperature, dim=1)\n",
        "\n",
        "            # Losses\n",
        "            loss_soft = kl_loss(s_log_probs, t_probs) * (temperature**2)\n",
        "            loss_hard = ce_loss(s_logits, y)\n",
        "            loss = alpha * loss_soft + (1 - alpha) * loss_hard\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Student Epoch {ep+1}: Loss = {total_loss/len(loader):.4f}\")"
      ],
      "metadata": {
        "id": "s5jjmP-HA8b3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distill(student, teacher, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niYrM947BE2e",
        "outputId": "9e425ccd-507c-4568-ae12-201945660df2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Epoch 1: Loss = 0.0795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate both Teacher and Student\n",
        "\n",
        "evaluate(teacher, test_loader, \"Teacher\")\n",
        "\n",
        "evaluate(student, test_loader, \"Student\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uU_qOxknBHLE",
        "outputId": "92ac849c-497f-4668-c412-98332d828ce5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Accuracy: 94.77%\n",
            "Student Accuracy: 95.76%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95.76"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQRbl7nWBXwI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}